{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxUIU3nOBr4Z"
      },
      "source": [
        "## Introduction <a name=\"introduction\"></a>\n",
        "SpamBase dataset is a classification dataset containing 4601 emails sent to HP (Hewlett-Packard) during some period of time. The SpamBase contains numeric 57 features for each email and a binary label, 1 for spam, 0 for ham(email). This is a typical binary classification problem with the added need for clever feature selection, as many of the features provided in the dataset might be useless (Hopkins et al., 1998)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKodxWoiBr4a"
      },
      "source": [
        "## Exploratory Data Analysis <a name=\"DataExploration\"></a>\n",
        "In this section, we use various python libraries like the pandas dataframe, numpy, matplotlib, and seaborn to do a data summary and data visualization of the spambase dataset. This will give us a better understanding of the distributions and relationships between various features and the target variable(label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihMyyo3eBr4b"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "#for data visualisation:\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrzeas0yDP--"
      },
      "outputs": [],
      "source": [
        "# read the file containing the column names\n",
        "with open('spambase.names') as f:\n",
        "    list_contents = f.readlines()\n",
        "    colnames = []\n",
        "    for item in list_contents:\n",
        "        colname = item.split(':')[0]\n",
        "        colnames.append(colname)\n",
        "colnames.append('label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xcqCUSNDVCO"
      },
      "outputs": [],
      "source": [
        "# get the length of the features(columns)\n",
        "len(colnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZLayvomDfP1"
      },
      "outputs": [],
      "source": [
        "# read the file containing the dataset and assign it to a variable\n",
        "dataset = pd.read_csv('spambase.data', header=None)\n",
        "\n",
        "dataset.columns = colnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzu0xZDyNYHE"
      },
      "outputs": [],
      "source": [
        "# get the first five rows of the dataset\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3CCLyVR1JrQ"
      },
      "outputs": [],
      "source": [
        "# get the last five rows of the dataset\n",
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VIbmGAaDkXy"
      },
      "outputs": [],
      "source": [
        "# check datatype for all columns and rows\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJxdmKZEDo-E"
      },
      "outputs": [],
      "source": [
        "#This gives us all the statistical summary for each column.\n",
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R08_rG2DobW"
      },
      "outputs": [],
      "source": [
        "# Get the shape of the dataset. the first element represents the number of rows and the second element represents the columns\n",
        "np.shape(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrZC8YBHDyww"
      },
      "outputs": [],
      "source": [
        "#check whether there are any missing or null values\n",
        "dataset.isnull().sum()#This will give number of NaN values in every column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW2eB4egDyoJ"
      },
      "outputs": [],
      "source": [
        "#If needed, check NaN values in every column using the code: by default axis=0\n",
        "dataset.isnull().sum(axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuEWlm1PDyTg"
      },
      "outputs": [],
      "source": [
        "# check if there's any duplicate row or column in the dataset \n",
        "dataset.duplicated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElwnRIzq9VoM"
      },
      "outputs": [],
      "source": [
        "# plot a histogram of the dataset.\n",
        "hist_of_dataset = dataset.hist(figsize = (30,20))\n",
        "hist_of_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GveG48osD9mD"
      },
      "outputs": [],
      "source": [
        "# visualize if there's any missing value using a barchart. A blue bar represents the number of missing values.\n",
        "# in this dataset we have no missing value.\n",
        "sns.set(rc={'figure.figsize':(17,7)})\n",
        "miss_vals = pd.DataFrame(dataset.isnull().sum() / len(dataset) * 100)\n",
        "miss_vals.plot(kind='bar',title='Missing values in percentage',ylabel='percentage')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRQUfIT4Br4c"
      },
      "source": [
        "### The Outcome <a name=\"Theoutcome\"></a>\n",
        "We've successfully analysed the spambase dataset using various exploratory tools like graphs and statistical analysis so as to enable us do an informed data preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAjY1HqSBr4d"
      },
      "source": [
        "## Data Preprocessing <a name=\"DataPreprocessing\"></a>\n",
        "In this section, we clean the dataset by filling the missing values(if any) with the mean of its column, we split our dataset into training and testing. We need to perform Feature Scaling when we are dealing with Gradient Descent Based algorithms. Scaling has no significant effect on tree based algorithms, so we would only scale it during the Neural network section. this is the section we also encode the target variable if it's not in binary format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OceT1WUFivyo"
      },
      "outputs": [],
      "source": [
        "# fill and missing data with te mean of its column.\n",
        "dataset.fillna(dataset.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8IpVoj30EY1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfj9R2itY0ZP"
      },
      "outputs": [],
      "source": [
        "# seperate the features from the target variable.\n",
        "X = dataset.drop('label', axis=1)\n",
        "y = dataset['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28bK2JWBUAqJ"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training set of 75% and test sets of 25%. We also set the stratify hyperparameter for equal distribution.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8kxyeXiolNC"
      },
      "outputs": [],
      "source": [
        "# get the shape of the split data\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_tJolNOBr4f"
      },
      "source": [
        "### The outcome <a name=\"Theoutcome\"></a>\n",
        "We've been able to fill missing data, and split our dataset into training and testing. scaling will be applied only to the neural network model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGSoGK4iQS7c"
      },
      "source": [
        ".\n",
        ".\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZZw_92JBr4f"
      },
      "source": [
        "## Feature Engineering and Feature Selection <a name=\"FEFS\"></a>\n",
        "Here we apply feature engineering and feature selection to get the relevant features. We achieve this by using sklearn variance threshold, pearson correlation coefficient, and DecisionTree Feature_importance_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lw-kb4xcPmO"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import VarianceThreshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV0-2Tn0L28w"
      },
      "source": [
        "Using scikit-learn Variance threshold to remove features with variance 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ_VmGv6L1al"
      },
      "outputs": [],
      "source": [
        "# check for variance within each feature and remove the features with variance=0\n",
        "var_thres = VarianceThreshold(threshold=0.0) # set the threshold to 0\n",
        "var_thres.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCpNQYDyegPQ"
      },
      "outputs": [],
      "source": [
        "# get the sum of all the features with a variance above 0 \n",
        "sum(var_thres.get_support())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSwUDtDTfCuf"
      },
      "outputs": [],
      "source": [
        "# we check how many features have a variance of 0\n",
        "constant_columns = [column for column in X_train.columns\n",
        "                    if column not in X_train.columns[var_thres.get_support()]]\n",
        "\n",
        "print(len(constant_columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9u2wjtrfk4Y"
      },
      "outputs": [],
      "source": [
        "# drop features with variance of 0. we apply it to only X_train not the whole dataset.\n",
        "X_train.drop(constant_columns,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5E6xJ_EMjZ4"
      },
      "source": [
        "Checking for correlation between features after applying the scikit learn variance threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssm-EkBlkQvI"
      },
      "outputs": [],
      "source": [
        "#we check with the X_train not the whole features.\n",
        "cor = X_train.corr()\n",
        "\n",
        "# we use the seaborn heatmap to visualise the correlations.\n",
        "cmap = sns.cm.rocket_r #for reversed color, the darker the more correlated.\n",
        "\n",
        "plt.figure(figsize=(50, 50)) # set the size of the plot.\n",
        "\n",
        "# initialise the seaborn heatmap.\n",
        "ax = sns.heatmap(cor, linewidths=.3, annot=True, fmt=\".2\", cmap=cmap)#show numbers on the cells: annot=True\n",
        "\n",
        "# for avoiding reseting labels\n",
        "ax.tick_params(axis='x', labelrotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jznrMWQ34lj3"
      },
      "source": [
        "Using Pearson Correlation Coefficient to remove features with high correlations between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIu1h7Uyaltu"
      },
      "outputs": [],
      "source": [
        "# code partially gotten from https://www.youtube.com/watch?v=FndwYNcVe0U&list=PLZoTAELRMXVPgjwJ8VyRoqmfNs2CJwhVH&index=3\n",
        "\n",
        "# Checking for correlation using pearson correlation coefficient\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()  # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)-1):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff values.\n",
        "                #We compare the feature correlation with target and drop the ones with lower coeff.\n",
        "                if abs(corr_matrix.iloc[j, len(corr_matrix.columns)-1]) > abs(corr_matrix.iloc[i, len(corr_matrix.columns)-1]):\n",
        "                    colname = corr_matrix.columns[i]\n",
        "                else:\n",
        "                    colname = corr_matrix.columns[j]\n",
        "                col_corr.add(colname)\n",
        "    return col_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB1c_TVtRFz8"
      },
      "outputs": [],
      "source": [
        "# Apply the pearson correlation to our dataset with a threshold of 90%.\n",
        "corr_features = correlation(dataset, 0.90)\n",
        "\n",
        "print(corr_features)\n",
        "\n",
        "print('number of correlated features: '+str(len(set(corr_features))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slDqGoqgqUI_"
      },
      "outputs": [],
      "source": [
        "# drop the correlated features. the features that are related to target are kept.\n",
        "dataset_feature_reduce = dataset.drop(corr_features, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZD82hJhcbva"
      },
      "outputs": [],
      "source": [
        "# get the shape of the reduced dataset\n",
        "dataset_feature_reduce.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24MzWmXnfjMl"
      },
      "outputs": [],
      "source": [
        "# get a new X and y from the reduced dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_feature_reduce.drop('label', axis=1), dataset_feature_reduce['label'],\n",
        "                                                    stratify=dataset_feature_reduce['label'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6CiUUWYhFlN"
      },
      "outputs": [],
      "source": [
        "len(X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhmrZBh0rgsf"
      },
      "outputs": [],
      "source": [
        "# visualise it with seaborn heatmap\n",
        "#reverse the color scheme: the darker the more positive related.\n",
        "cmap = sns.cm.rocket_r \n",
        "\n",
        "plt.figure(figsize=(50, 50)) # set the size of the heatmap\n",
        "\n",
        "#https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
        "view = sns.heatmap(dataset_feature_reduce.corr(), linewidths=.3, annot=True, fmt=\".2\", cmap=cmap)#show numbers on the cells: annot=True\n",
        "\n",
        "# To avoid resetting labels\n",
        "view.tick_params(axis='x', labelrotation=45) # tilt the x-label by 45 degree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0K1IGfOeGzn"
      },
      "source": [
        "Let's compare the evaluation results before and after reducing features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnvViR3e0MIb",
        "outputId": "606dc665-53dd-4720-d9cd-ad127e0345f9"
      },
      "outputs": [],
      "source": [
        "#before feature reduction\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_rf = RandomForestClassifier(n_estimators=100)#todo: tune hyper param\n",
        "scores_rf_featRed = cross_val_score(model_rf, dataset.drop('label', axis=1), dataset_feature_reduce['label'], scoring = \"f1_weighted\", cv=5)\n",
        "print(scores_rf_featRed)\n",
        "scores_rf_featRed.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A_9ZyX75VCD",
        "outputId": "3a09bacf-192a-4ffc-94b2-5f759f52c378"
      },
      "outputs": [],
      "source": [
        "#after feature reduction\n",
        "model_rf = RandomForestClassifier(n_estimators=100)\n",
        "scores_rf_featRed = cross_val_score(model_rf, dataset_feature_reduce.drop('label', axis=1), dataset_feature_reduce['label'], scoring = \"f1_weighted\", cv=5)\n",
        "print(scores_rf_featRed)\n",
        "scores_rf_featRed.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpWpd9FB6mxs"
      },
      "source": [
        "Applying Adaboost before and after feature reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPJJ1Qfw6kkW",
        "outputId": "327be985-a261-440c-ead3-62ca6d31b62d"
      },
      "outputs": [],
      "source": [
        "#before feature reduction\n",
        "# use the AdaBoost classifier with the default base classifier - DecisionTreeClassifier(max_depth=1) \n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model_ada = AdaBoostClassifier(n_estimators=100)#todo: tune hyper param\n",
        "scores_ada_feature_reduce = cross_val_score(model_ada, dataset.drop('label', axis=1), dataset_feature_reduce['label'], scoring = \"f1_weighted\", cv=5)\n",
        "print(scores_ada_feature_reduce)\n",
        "scores_ada_feature_reduce.mean()#best result: 0.9423"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pwyo8H-j64yc",
        "outputId": "685ee7f4-0e29-4e51-a0ff-b2a3b5661292"
      },
      "outputs": [],
      "source": [
        "#after feature reduction\n",
        "model_ada = AdaBoostClassifier(n_estimators=100)\n",
        "scores_ada_feature_reduce = cross_val_score(model_ada, dataset_feature_reduce.drop('label', axis=1), dataset_feature_reduce['label'], scoring = \"f1_weighted\", cv=5)\n",
        "print(scores_ada_feature_reduce)\n",
        "scores_ada_feature_reduce.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUV0-QmkerC-"
      },
      "source": [
        "###The Outcome\n",
        "There's no significant difference before and after dropping the feature because there's only one feature dropped and it was insignifiant to the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yfYgk1GfGGI"
      },
      "source": [
        "Sorting the features using the feature_importance_ and removing the features with importance <= 0 in relation to the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOYFkxiXBr4g"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42, max_depth=8,class_weight='balanced') \n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYA9AITxhsME"
      },
      "outputs": [],
      "source": [
        "# View the important feature on a barplot\n",
        "feat_importances = pd.DataFrame(importances, index=X_train.columns, columns=[\"Importance\"])\n",
        "feat_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
        "feat_importances.plot(kind='bar', figsize=(16,7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA2pWX0FXVTT"
      },
      "outputs": [],
      "source": [
        "# Append the features greater than 0 to a new array\n",
        "threshold = 0.00000\n",
        "important_features = [feature for feature, importance in zip(X_train.columns, importances) if importance > threshold]\n",
        "\n",
        "important_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7mXAT5xWpbg"
      },
      "outputs": [],
      "source": [
        "# get the length of the new important features\n",
        "len(important_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDBCw3QLqyUL"
      },
      "outputs": [],
      "source": [
        "# create a new dataset with only the important features\n",
        "import_feat = dataset_feature_reduce[important_features]\n",
        "import_feat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac-toMGUincr"
      },
      "outputs": [],
      "source": [
        "cmap = sns.cm.rocket_r #reverse the color scheme: the darker the more positive related\n",
        "plt.figure(figsize=(60, 50))\n",
        "\n",
        "# Plot a heatmap of the important features\n",
        "heat = sns.heatmap(import_feat.corr(), linewidths=.3, annot=True, fmt=\".2\", cmap=cmap)\n",
        "\n",
        "heat.tick_params(axis='x', labelrotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S-KOlbjBr4g"
      },
      "source": [
        "### The outcome <a name=\"Theoutcome\"></a>\n",
        "In this example, we've applied sklearn variance threshold, pearson corr. coef. and decision tree feature_importance_ to extract the features that are relevant in predicting the outcome of an email."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYDQ9vSRBr4h"
      },
      "source": [
        "## Select, Train, Apply ML models <a name=\"SA\"></a>\n",
        "In this section, we'll explore, compare and optimise various classification models,ensemble models and ANN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7mN9SlnfXN"
      },
      "source": [
        "Using Support Vector Machine on the reduced spambase dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Noil0VXnbQy"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score # to check the accuracy of the model\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(import_feat, y, stratify=y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train the SVM model\n",
        "clf = svm.SVC(kernel='linear') #Linear Kernel is used when the data is Linearly separable \n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL0wzkG9GBTX"
      },
      "source": [
        "Using Decision tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvXsXM_VF_NK"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dtc = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dtc.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC2ZE-rUGa18"
      },
      "source": [
        "Using SGClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRGEDhOrGZZP",
        "outputId": "d0c2ab8f-d093-4c75-ec5d-fc1144c30a90"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(loss = 'modified_huber') # \"modified_humber\" brings tolerance to outliers as well as probability estimates.\n",
        "\n",
        "sgd_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dtc.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mh9C_9WogUx"
      },
      "source": [
        "Using Random Forest to classify the Spambase Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP-BAbCSoqzF",
        "outputId": "860e531a-47a5-45eb-b986-4114440d7e04"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train the random forest model\n",
        "clf = RandomForestClassifier(n_estimators=100,max_depth=5, random_state=42)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(acc * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyI93ssux6my"
      },
      "source": [
        "Using Ensemble learning techniques boosting(Adaboost Classifier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dldfedBa3HtH",
        "outputId": "00d3cd30-9a8c-440d-89b5-7a645715b3db"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Train the AdaBoost model\n",
        "ada_clf = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42) #Weight applied to each classifier at each boosting iteration\n",
        "\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = ada_clf.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g853t4YKs-Ux"
      },
      "source": [
        "Using Ensemble learning Bagging Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3foVoq57NjZ",
        "outputId": "f204d4c6-a691-461b-88b8-fbdd89c573e1"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# Define the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
        "\n",
        "# Train the Bagging model\n",
        "bag_clf = BaggingClassifier(estimator=base_estimator, n_estimators=100, random_state=42)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = bag_clf.score(X_test, y_test)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac4mvVqpoQLx"
      },
      "source": [
        "In this section, we use Artificial Neural Network(ANN) to classify the Spambase Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idiH4ZoJBr4h"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOrc-gQ-iuU5"
      },
      "outputs": [],
      "source": [
        "sc = MinMaxScaler() # define the scaler\n",
        "df_scaled = pd.DataFrame(sc.fit_transform(import_feat)) # fit & transform the data\n",
        "print(df_scaled.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVa3aT5Rm-BL"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_scaled, y, stratify=y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUa3iclvW5XS"
      },
      "outputs": [],
      "source": [
        "# initialize the neural network\n",
        "neu_net = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4,\n",
        "                        solver='sgd', verbose=10, tol=1e-4, random_state=1, # solver specifies the algorithm for weight optimization over the nodes.\n",
        "                        learning_rate_init=.1) \n",
        "\n",
        "# train the neural network\n",
        "neu_net.fit(X_train, y_train)\n",
        "\n",
        "# evaluate the model\n",
        "y_pred = neu_net.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5WgYU3eBr4h"
      },
      "source": [
        "### The outcome <a name=\"Theoutcome\"></a>\n",
        "We were able to train and test our dataset using various classifiers and ensemble techniques, and each one performed well, some better than others. Now we would evaluate them to know which one performed better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKN6j4GaBr4i"
      },
      "source": [
        "## Evaluation <a name=\"Evaluation\"></a>\n",
        "Evaluate the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-51F_NMBr4i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the models to evaluate\n",
        "models = {\"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "          \"Decision Tree\":DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "          \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "          \"SVM\":svm.SVC(kernel='linear'),\n",
        "          \"SGD\":SGDClassifier(loss = 'modified_huber'),\n",
        "          \"Bagging\": BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=5, random_state=42), n_estimators=100, random_state=42),\n",
        "          \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4,\n",
        "                        solver='sgd', verbose=10, tol=1e-4, random_state=1, learning_rate_init=.1)\n",
        "         }\n",
        "\n",
        "# Evaluate each model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "    precision = precision_score(y_test, y_pred) * 100\n",
        "    recall = recall_score(y_test, y_pred) * 100\n",
        "    f1 = f1_score(y_test, y_pred) * 100\n",
        "    print(f\"{name}:\\n\\tAccuracy: {accuracy:.2f}\\n\\tPrecision: {precision:.2f}\\n\\tRecall: {recall:.2f}\\n\\tF1-Score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7avrfGeBr4i"
      },
      "source": [
        "## Communicating the Results <a name=\"CommunicateResults\"></a>\n",
        "After applying data preprocessing, feature engineering and selection, training and testing of the spambase dataset, both individually and combined through the evaluation process, the model with the highest precision scores are Adaboost and random forest with an accuracy of 94.61 and 94.53 respectively. Other models performed well like the ANN with a score of 92.44. A decrease in the learning rate of the ANN to 0.01 showed a slight improvement to 92.96 but it took a longer time to execute. SVM had the least accuracy score of 89.04.\n",
        "It is recommended that Adaboost should be used when building a model to classify emails as Spam or Ham as its accuracy is higher than other classifiers."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
